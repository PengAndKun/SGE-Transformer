{
 "cells": [
  {
   "cell_type": "code",
   "id": "a3de016e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# 完整：重构全部轨迹 -> 保存缓存 -> 读取缓存 -> 随机展示一条(states/actions/rewards)\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ========== 1) import 路径 & pickle shim ==========\n",
    "repo_root = Path(os.getcwd()).resolve().parents[1]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# from ISE_Transformer.DT_test.DT_train import _install_go_explore_pickle_shim\n",
    "# _install_go_explore_pickle_shim()\n",
    "\n",
    "from ISE_Transformer.experiment.il_2_room.environment import GridWorld\n",
    "\n",
    "# ========== 10_e) 配置 ==========\n",
    "PKL_IN = Path(\"../data_room/2_room/optimal_trajectory_archive2.pkl\").resolve()\n",
    "assert PKL_IN.exists(), f\"not found: {PKL_IN}\"\n",
    "\n",
    "# 缓存文件（你要“先保存下来，再读取后随机展示一条”，就写到这个新文件里）\n",
    "CACHE_OUT = Path(\"./reconstructed_cache_all_trajs.pkl\").resolve()\n",
    "\n",
    "# DT动作(数据集/go_explore_env): 0=up,1=down,10_e=left,3=right\n",
    "# GridWorld动作(environment.GridWorld): 1=right,10_e=up,3=left,4=down\n",
    "DT2GW = torch.tensor([2, 4, 3, 1], dtype=torch.long)\n",
    "\n",
    "ALG_TYPE = \"NM\"      # weighted_traj_return 的 type（对齐 DT_test.ipynb）\n",
    "RNG_SEED_SHOW = None    # 控制“展示哪一条”的随机种子\n",
    "\n",
    "# ========== 3) helper ==========\n",
    "def make_params(batch_size: int = 1, horizon: int = 80, node_weight: str = \"constant\", initial: int = 80):\n",
    "    return {\n",
    "        \"env\": {\n",
    "            \"start\": 1,\n",
    "            \"step_size\": 0.1,\n",
    "            \"shape\": {\"x\": 11, \"y\": 18},\n",
    "            \"horizon\": int(horizon),\n",
    "            \"node_weight\": str(node_weight),\n",
    "            \"disc_size\": \"small\",\n",
    "            \"n_players\": 3,\n",
    "            \"Cx_lengthscale\": 2,\n",
    "            \"Cx_noise\": 0.001,\n",
    "            \"Fx_lengthscale\": 1,\n",
    "            \"Fx_noise\": 0.001,\n",
    "            \"Cx_beta\": 1.5,\n",
    "            \"Fx_beta\": 1.5,\n",
    "            \"generate\": False,\n",
    "            \"env_file_name\": \"env_data.pkl\",\n",
    "            \"cov_module\": \"Matern\",\n",
    "            \"stochasticity\": 0.0,\n",
    "            \"domains\": \"two_room_2\",\n",
    "            \"num\": 1,\n",
    "            \"initial\": int(initial),\n",
    "        },\n",
    "        \"common\": {\n",
    "            \"a\": 1,\n",
    "            \"subgrad\": \"greedy\",\n",
    "            \"grad\": \"pytorch\",\n",
    "            \"algo\": \"both\",\n",
    "            \"init\": \"deterministic\",\n",
    "            \"batch_size\": int(batch_size),\n",
    "        },\n",
    "        \"visu\": {\"wb\": \"disabled\", \"a\": 1},\n",
    "        \"alg\": {\"type\": str(ALG_TYPE), \"gamma\": 1},\n",
    "    }\n",
    "\n",
    "def make_gridworld_env(params):\n",
    "    env_load_path = (\n",
    "        Path(\"../..\")\n",
    "        / \"experiment\"\n",
    "        / \"il_2_room\"\n",
    "        / \"2r198\"\n",
    "        / \"environments\"\n",
    "        / params[\"env\"][\"node_weight\"]\n",
    "        / \"env_1\"\n",
    "    ).resolve()\n",
    "\n",
    "    env = GridWorld(\n",
    "        env_params=params[\"env\"],\n",
    "        common_params=params[\"common\"],\n",
    "        visu_params=params[\"visu\"],\n",
    "        env_file_path=str(env_load_path),\n",
    "    )\n",
    "    # 对齐脚本初始化流程\n",
    "    env.common_params[\"batch_size\"] = int(params[\"common\"][\"batch_size\"])\n",
    "    env.initialize(params[\"env\"][\"initial\"])\n",
    "    env.get_horizon_transition_matrix()\n",
    "    return env\n",
    "\n",
    "def get_state_tensor(env) -> torch.LongTensor:\n",
    "    s = env.state\n",
    "    if not torch.is_tensor(s):\n",
    "        s = torch.as_tensor(s)\n",
    "    return s.long().view(-1)  # (B,)\n",
    "\n",
    "def weighted_prefix_rewards(env, mat_state, alg_type: str) -> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    用 weighted_traj_return 的“前缀增量”构造逐步 rewards：\n",
    "      R_t = return(s0..s_t)\n",
    "      rewards[t] = (R_{t+1}-R_t)\n",
    "    并做基线平移：R_t -= R_0，保证 sum(rewards) == traj_return\n",
    "    \"\"\"\n",
    "    T = len(mat_state) - 1  # s0..sT，共 T+1 个\n",
    "    prefix = np.empty((T + 1,), dtype=np.float32)\n",
    "    for t_end in range(T + 1):\n",
    "        obj = env.weighted_traj_return(mat_state[: t_end + 1], type=alg_type).float()  # (B=1,)\n",
    "        prefix[t_end] = float(obj.view(-1)[0].item())\n",
    "    prefix = prefix - prefix[0]\n",
    "    rewards = np.diff(prefix).astype(np.float32)  # (T,)\n",
    "    traj_return = float(prefix[-1])\n",
    "    return rewards, traj_return\n",
    "\n",
    "# ========== 4) 如果缓存不存在：重构全部并保存 ==========\n",
    "if not CACHE_OUT.exists():\n",
    "    print(\"[INFO] cache not found, reconstructing ALL trajectories...\")\n",
    "    with open(PKL_IN, \"rb\") as f:\n",
    "        archive = pickle.load(f)\n",
    "    if not isinstance(archive, dict):\n",
    "        raise TypeError(f\"Unexpected pkl type: {type(archive)} (expected dict)\")\n",
    "\n",
    "    items = sorted(list(archive.items()), key=lambda kv: str(kv[0]))\n",
    "\n",
    "    trajectories = []\n",
    "    skipped = 0\n",
    "    processed = 0\n",
    "\n",
    "    for idx, (k, node) in enumerate(items):\n",
    "        actions_dt_list = list(getattr(node, \"path_actions\", []) or [])\n",
    "        if len(actions_dt_list) == 0:\n",
    "            continue\n",
    "\n",
    "        a_dt_np = np.asarray(actions_dt_list, dtype=np.int64)\n",
    "        if a_dt_np.min() < 0 or a_dt_np.max() > 3:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        a_dt = torch.tensor(a_dt_np.tolist(), dtype=torch.long)\n",
    "        a_gw = DT2GW[a_dt]  # (T,)\n",
    "        T = int(a_dt.numel())\n",
    "\n",
    "        params = make_params(batch_size=1, horizon=T + 1)\n",
    "        env = make_gridworld_env(params)\n",
    "\n",
    "        # replay 重构 mat_state: [s0..sT]\n",
    "        mat_state = [get_state_tensor(env).clone()]  # s0\n",
    "        for t in range(T):\n",
    "            env.step(int(t), torch.tensor([int(a_gw[t].item())], dtype=torch.long))\n",
    "            mat_state.append(get_state_tensor(env).clone())\n",
    "\n",
    "        states = torch.cat(mat_state, dim=0).detach().cpu().numpy().astype(np.int64)  # (T+1,)\n",
    "        actions_dt = a_dt.detach().cpu().numpy().astype(np.int64)                    # (T,)\n",
    "        actions_gw = a_gw.detach().cpu().numpy().astype(np.int64)                    # (T,)\n",
    "\n",
    "        rewards, traj_return = weighted_prefix_rewards(env, mat_state, alg_type=params[\"alg\"][\"type\"])\n",
    "\n",
    "        trajectories.append(\n",
    "            {\n",
    "                \"key\": str(k),\n",
    "                \"states\": states,              # list/np array, len=T+1\n",
    "                \"actions_dt\": actions_dt,      # len=T, 0..3\n",
    "                \"actions_gw\": actions_gw,      # len=T, 1..4\n",
    "                \"rewards\": rewards,            # len=T, sum == traj_return\n",
    "                \"traj_return\": np.float32(traj_return),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 200 == 0:\n",
    "            print(f\"[INFO] reconstructed {processed} trajectories... (scanned {idx+1}/{len(items)})\")\n",
    "\n",
    "    payload = {\n",
    "        \"source_pkl\": str(PKL_IN),\n",
    "        \"cache_out\": str(CACHE_OUT),\n",
    "        \"alg_type\": ALG_TYPE,\n",
    "        \"DT2GW\": DT2GW.cpu().numpy().tolist(),\n",
    "        \"format\": {\n",
    "            \"states\": \"GridWorld state_id (int64), len=T+1\",\n",
    "            \"actions_dt\": \"dataset action (int64) in [0,3], len=T\",\n",
    "            \"actions_gw\": \"GridWorld action (int64) in [1,4], len=T\",\n",
    "            \"rewards\": \"per-step rewards from weighted_traj_return prefix-diff (float32), len=T\",\n",
    "            \"traj_return\": \"float32, equals sum(rewards)\",\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"num_items_in_archive\": len(items),\n",
    "            \"num_reconstructed\": len(trajectories),\n",
    "            \"num_skipped_invalid_actions\": skipped,\n",
    "        },\n",
    "        \"trajectories\": trajectories,\n",
    "    }\n",
    "\n",
    "    with open(CACHE_OUT, \"wb\") as f:\n",
    "        pickle.dump(payload, f)\n",
    "\n",
    "    print(\"[OK] saved cache:\", CACHE_OUT)\n",
    "else:\n",
    "    print(\"[INFO] cache exists, skip reconstruction:\", CACHE_OUT)\n",
    "\n",
    "# ========== 5) 读取缓存并随机展示一条 ==========\n",
    "with open(CACHE_OUT, \"rb\") as f:\n",
    "    cache = pickle.load(f)\n",
    "\n",
    "trajs = cache.get(\"trajectories\", [])\n",
    "if not trajs:\n",
    "    raise RuntimeError(\"cache has no trajectories\")\n",
    "\n",
    "rng = random.Random(RNG_SEED_SHOW)\n",
    "one = rng.choice(trajs)\n",
    "\n",
    "states = np.asarray(one[\"states\"], dtype=np.int64)\n",
    "actions_dt = np.asarray(one[\"actions_dt\"], dtype=np.int64)\n",
    "actions_gw = np.asarray(one[\"actions_gw\"], dtype=np.int64)\n",
    "rewards = np.asarray(one[\"rewards\"], dtype=np.float32)\n",
    "traj_return = float(one.get(\"traj_return\", rewards.sum()))\n",
    "\n",
    "print(\"\\n[SUMMARY]\")\n",
    "print(\"cache:\", str(CACHE_OUT))\n",
    "print(\"num_trajectories:\", len(trajs))\n",
    "print(\"alg_type:\", cache.get(\"alg_type\"))\n",
    "\n",
    "print(\"\\n[RANDOM TRAJ]\")\n",
    "print(\"key =\", one.get(\"key\"))\n",
    "print(\"T =\", int(actions_dt.shape[0]))\n",
    "print(\"traj_return =\", traj_return)\n",
    "print(\"check sum(rewards) =\", float(rewards.sum()))\n",
    "\n",
    "print(\"\\nstates(list) =\")\n",
    "print(states.tolist())\n",
    "\n",
    "print(\"\\nactions_dt(list, 0..3) =\")\n",
    "print(actions_dt.tolist())\n",
    "\n",
    "print(\"\\nactions_gw(list, 1..4) =\")\n",
    "print(actions_gw.tolist())\n",
    "\n",
    "print(\"\\nrewards(list, per-step; sum == traj_return) =\")\n",
    "print(rewards.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "744bd664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device = cuda\n",
      "[INFO] num_trajs = 112 alg_type(cache) = NM\n",
      "[INFO] state_vocab = 198 max_timestep = 85\n",
      "[INFO] DESIRED_RETURN = 144.0\n",
      "[TRAIN] step=   50 loss=0.651229  test_return=65.000000\n",
      "[TRAIN] step=  100 loss=0.450928  test_return=106.000000\n",
      "[TRAIN] step=  150 loss=0.550896  test_return=144.000000\n",
      "[TRAIN] step=  200 loss=0.298260  test_return=137.000000\n",
      "[TRAIN] step=  250 loss=0.312202  test_return=133.000000\n",
      "[TRAIN] step=  300 loss=0.241559  test_return=134.000000\n",
      "[TRAIN] step=  350 loss=0.323345  test_return=144.000000\n",
      "[TRAIN] step=  400 loss=0.152591  test_return=144.000000\n",
      "[TRAIN] step=  450 loss=0.140766  test_return=140.000000\n",
      "[TRAIN] step=  500 loss=0.160854  test_return=142.000000\n",
      "[TRAIN] step=  550 loss=0.159200  test_return=137.000000\n",
      "[TRAIN] step=  600 loss=0.133808  test_return=144.000000\n",
      "[TRAIN] step=  650 loss=0.156491  test_return=140.000000\n",
      "[TRAIN] step=  700 loss=0.085428  test_return=144.000000\n",
      "[TRAIN] step=  750 loss=0.074581  test_return=144.000000\n",
      "[TRAIN] step=  800 loss=0.099611  test_return=144.000000\n",
      "[TRAIN] step=  850 loss=0.102280  test_return=143.000000\n",
      "[TRAIN] step=  900 loss=0.085627  test_return=136.000000\n",
      "[TRAIN] step=  950 loss=0.111412  test_return=144.000000\n",
      "[TRAIN] step= 1000 loss=0.096849  test_return=144.000000\n",
      "[TRAIN] step= 1050 loss=0.187054  test_return=140.000000\n",
      "[TRAIN] step= 1100 loss=0.057555  test_return=144.000000\n",
      "[TRAIN] step= 1150 loss=0.138385  test_return=144.000000\n",
      "[TRAIN] step= 1200 loss=0.083331  test_return=144.000000\n",
      "[TRAIN] step= 1250 loss=0.051503  test_return=144.000000\n",
      "[TRAIN] step= 1300 loss=0.060243  test_return=144.000000\n",
      "[TRAIN] step= 1350 loss=0.087848  test_return=144.000000\n",
      "[TRAIN] step= 1400 loss=0.089738  test_return=144.000000\n",
      "[TRAIN] step= 1450 loss=0.093804  test_return=144.000000\n",
      "[TRAIN] step= 1500 loss=0.054345  test_return=144.000000\n",
      "[TRAIN] step= 1550 loss=0.081319  test_return=140.000000\n",
      "[TRAIN] step= 1600 loss=0.079997  test_return=144.000000\n",
      "[TRAIN] step= 1650 loss=0.128611  test_return=144.000000\n",
      "[TRAIN] step= 1700 loss=0.109299  test_return=144.000000\n",
      "[TRAIN] step= 1750 loss=0.065137  test_return=144.000000\n",
      "[TRAIN] step= 1800 loss=0.042309  test_return=144.000000\n",
      "[TRAIN] step= 1850 loss=0.096438  test_return=144.000000\n",
      "[TRAIN] step= 1900 loss=0.076144  test_return=144.000000\n",
      "[TRAIN] step= 1950 loss=0.102029  test_return=144.000000\n",
      "[TRAIN] step= 2000 loss=0.076607  test_return=144.000000\n",
      "[TRAIN] step= 2050 loss=0.047010  test_return=144.000000\n",
      "[TRAIN] step= 2100 loss=0.097172  test_return=144.000000\n",
      "[TRAIN] step= 2150 loss=0.090493  test_return=144.000000\n",
      "[TRAIN] step= 2200 loss=0.054579  test_return=144.000000\n",
      "[TRAIN] step= 2250 loss=0.106636  test_return=143.000000\n",
      "[TRAIN] step= 2300 loss=0.054340  test_return=143.000000\n",
      "[TRAIN] step= 2350 loss=0.090108  test_return=144.000000\n",
      "[TRAIN] step= 2400 loss=0.057291  test_return=144.000000\n",
      "[TRAIN] step= 2450 loss=0.072461  test_return=144.000000\n",
      "[TRAIN] step= 2500 loss=0.065868  test_return=144.000000\n",
      "[TRAIN] step= 2550 loss=0.060051  test_return=144.000000\n",
      "[TRAIN] step= 2600 loss=0.070856  test_return=144.000000\n",
      "[TRAIN] step= 2650 loss=0.062959  test_return=144.000000\n",
      "[TRAIN] step= 2700 loss=0.062879  test_return=144.000000\n",
      "[TRAIN] step= 2750 loss=0.078649  test_return=144.000000\n",
      "[TRAIN] step= 2800 loss=0.086984  test_return=144.000000\n",
      "[TRAIN] step= 2850 loss=0.033931  test_return=144.000000\n",
      "[TRAIN] step= 2900 loss=0.051993  test_return=144.000000\n",
      "[TRAIN] step= 2950 loss=0.126748  test_return=144.000000\n",
      "[TRAIN] step= 3000 loss=0.062631  test_return=144.000000\n",
      "[OK] saved: D:\\Users\\ZHY\\Documents\\GitHub\\SGE-Transformer\\SGE_Transformer\\DT_test\\dt_full_ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "import pickle, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ISE_Transformer.experiment.il_2_room.environment import GridWorld\n",
    "\n",
    "# ===================== 配置 =====================\n",
    "CACHE_OUT = Path(\"./reconstructed_cache_all_trajs.pkl\").resolve()\n",
    "assert CACHE_OUT.exists(), f\"cache not found: {CACHE_OUT}\"\n",
    "\n",
    "# 数据动作空间：actions_dt in [0,3]\n",
    "NUM_ACTIONS = 4\n",
    "ACTION_PAD = NUM_ACTIONS  # PAD id = 4\n",
    "\n",
    "# dt动作(0..3) -> GridWorld动作(1..4)\n",
    "DT2GW = torch.tensor([2, 4, 3, 1], dtype=torch.long)\n",
    "\n",
    "K = 30                    # context length（每条轨迹随机截取 K）\n",
    "BATCH_SIZE = 8            # 每 step 采样多少条“随机轨迹窗口”（要严格每次一条就设 1）\n",
    "TRAIN_STEPS = 3000\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "D_MODEL = 128\n",
    "N_HEAD = 4\n",
    "N_LAYER = 4\n",
    "DROPOUT = 0.1\n",
    "RTG_SCALE = 100.0         # RTG 缩放\n",
    "SEED = None               # 设整数可复现\n",
    "\n",
    "# 评估：每隔多少 step rollout 一次\n",
    "EVAL_EVERY = 50\n",
    "EVAL_HORIZON = 80         # rollout 最长步数（含初始状态约为 T+1 状态）\n",
    "\n",
    "# weighted_traj_return 的 type（与你重构时一致）\n",
    "ALG_TYPE = \"NM\"\n",
    "\n",
    "SAVE_PATH = Path(\"./dt_full_ckpt.pt\").resolve()\n",
    "\n",
    "# ===================== 随机种子 =====================\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] device =\", device)\n",
    "\n",
    "# ===================== 读取缓存 =====================\n",
    "with open(CACHE_OUT, \"rb\") as f:\n",
    "    cache = pickle.load(f)\n",
    "trajs = cache.get(\"trajectories\", [])\n",
    "assert len(trajs) > 0, \"cache has no trajectories\"\n",
    "print(\"[INFO] num_trajs =\", len(trajs), \"alg_type(cache) =\", cache.get(\"alg_type\"))\n",
    "\n",
    "# state_vocab / max_timestep\n",
    "max_state_id = 0\n",
    "max_T = 0\n",
    "traj_returns = []\n",
    "for tr in trajs:\n",
    "    s = np.asarray(tr[\"states\"], dtype=np.int64)\n",
    "    a = np.asarray(tr[\"actions_dt\"], dtype=np.int64)\n",
    "    max_state_id = max(max_state_id, int(s.max()))\n",
    "    max_T = max(max_T, int(a.shape[0]))\n",
    "    if \"traj_return\" in tr:\n",
    "        traj_returns.append(float(tr[\"traj_return\"]))\n",
    "state_vocab = max_state_id + 1\n",
    "max_timestep = max(max_T, EVAL_HORIZON, K) + 5\n",
    "print(\"[INFO] state_vocab =\", state_vocab, \"max_timestep =\", max_timestep)\n",
    "\n",
    "# 评估时的目标 return（用数据集中较高的 return 做 conditioning）\n",
    "if len(traj_returns) > 0:\n",
    "    DESIRED_RETURN = float(np.percentile(np.array(traj_returns, dtype=np.float32), 90))\n",
    "else:\n",
    "    DESIRED_RETURN = 1.0\n",
    "print(\"[INFO] DESIRED_RETURN =\", DESIRED_RETURN)\n",
    "\n",
    "# ===================== 环境构造（用于评估）=====================\n",
    "def make_params(batch_size: int = 1, horizon: int = 80, node_weight: str = \"constant\", initial: int = 80):\n",
    "    return {\n",
    "        \"env\": {\n",
    "            \"start\": 1,\n",
    "            \"step_size\": 0.1,\n",
    "            \"shape\": {\"x\": 11, \"y\": 18},\n",
    "            \"horizon\": int(horizon),\n",
    "            \"node_weight\": str(node_weight),\n",
    "            \"disc_size\": \"small\",\n",
    "            \"n_players\": 3,\n",
    "            \"Cx_lengthscale\": 2,\n",
    "            \"Cx_noise\": 0.001,\n",
    "            \"Fx_lengthscale\": 1,\n",
    "            \"Fx_noise\": 0.001,\n",
    "            \"Cx_beta\": 1.5,\n",
    "            \"Fx_beta\": 1.5,\n",
    "            \"generate\": False,\n",
    "            \"env_file_name\": \"env_data.pkl\",\n",
    "            \"cov_module\": \"Matern\",\n",
    "            \"stochasticity\": 0.0,\n",
    "            \"domains\": \"two_room_2\",\n",
    "            \"num\": 1,\n",
    "            \"initial\": int(initial),\n",
    "        },\n",
    "        \"common\": {\n",
    "            \"a\": 1,\n",
    "            \"subgrad\": \"greedy\",\n",
    "            \"grad\": \"pytorch\",\n",
    "            \"algo\": \"both\",\n",
    "            \"init\": \"deterministic\",\n",
    "            \"batch_size\": int(batch_size),\n",
    "        },\n",
    "        \"visu\": {\"wb\": \"disabled\", \"a\": 1},\n",
    "        \"alg\": {\"type\": str(ALG_TYPE), \"gamma\": 1},\n",
    "    }\n",
    "\n",
    "def make_gridworld_env(params):\n",
    "    env_load_path = (\n",
    "        Path(\"../..\")\n",
    "        / \"experiment\"\n",
    "        / \"il_2_room\"\n",
    "        / \"2r198\"\n",
    "        / \"environments\"\n",
    "        / params[\"env\"][\"node_weight\"]\n",
    "        / \"env_1\"\n",
    "    ).resolve()\n",
    "\n",
    "    env = GridWorld(\n",
    "        env_params=params[\"env\"],\n",
    "        common_params=params[\"common\"],\n",
    "        visu_params=params[\"visu\"],\n",
    "        env_file_path=str(env_load_path),\n",
    "    )\n",
    "    env.common_params[\"batch_size\"] = int(params[\"common\"][\"batch_size\"])\n",
    "    env.initialize(params[\"env\"][\"initial\"])\n",
    "    env.get_horizon_transition_matrix()\n",
    "    return env\n",
    "\n",
    "def get_state_id(env) -> int:\n",
    "    s = env.state\n",
    "    if torch.is_tensor(s):\n",
    "        s = int(s.view(-1)[0].item())\n",
    "    else:\n",
    "        s = int(np.asarray(s).reshape(-1)[0])\n",
    "    return s\n",
    "\n",
    "def weighted_return_baselined(env, mat_state, alg_type: str) -> float:\n",
    "    # return([s0..sT]) - return([s0])\n",
    "    objT = env.weighted_traj_return(mat_state, type=alg_type).float().view(-1)[0].item()\n",
    "    obj0 = env.weighted_traj_return(mat_state[:1], type=alg_type).float().view(-1)[0].item()\n",
    "    return float(objT - obj0)\n",
    "\n",
    "# ===================== 轨迹采样（每次随机取一条轨迹窗口）=====================\n",
    "def make_rtg(rewards: np.ndarray) -> np.ndarray:\n",
    "    return np.cumsum(rewards[::-1], dtype=np.float32)[::-1].astype(np.float32)\n",
    "\n",
    "def sample_batch(trajs, batch_size: int, K: int, rtg_scale: float, device: torch.device):\n",
    "    states_b, prev_a_b, target_a_b, rtg_b, t_b, keymask_b, lossmask_b = [], [], [], [], [], [], []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        tr = random.choice(trajs)  # 每次随机取一条轨迹\n",
    "        states_all = np.asarray(tr[\"states\"], dtype=np.int64)       # (T+1,)\n",
    "        actions = np.asarray(tr[\"actions_dt\"], dtype=np.int64)      # (T,)\n",
    "        rewards = np.asarray(tr[\"rewards\"], dtype=np.float32)       # (T,)\n",
    "\n",
    "        T = int(actions.shape[0])\n",
    "        states = states_all[:-1]  # (T,)\n",
    "\n",
    "        rtg = make_rtg(rewards) / float(rtg_scale)  # (T,)\n",
    "\n",
    "        if T >= K:\n",
    "            start = random.randint(0, T - K)\n",
    "            end = start + K\n",
    "            pad = 0\n",
    "            s = states[start:end]\n",
    "            a = actions[start:end]\n",
    "            r = rtg[start:end]\n",
    "            tt = np.arange(start, end, dtype=np.int64)\n",
    "            valid_len = K\n",
    "        else:\n",
    "            pad = K - T\n",
    "            s = np.pad(states, (0, pad), constant_values=0)\n",
    "            a = np.pad(actions, (0, pad), constant_values=0)\n",
    "            r = np.pad(rtg, (0, pad), constant_values=0.0)\n",
    "            tt = np.pad(np.arange(0, T, dtype=np.int64), (0, pad), constant_values=0)\n",
    "            valid_len = T\n",
    "\n",
    "        # prev_actions: [PAD] + a[:-1]\n",
    "        prev = np.empty((K,), dtype=np.int64)\n",
    "        prev[0] = ACTION_PAD\n",
    "        prev[1:] = a[:-1]\n",
    "        if pad > 0:\n",
    "            prev[-pad:] = ACTION_PAD\n",
    "\n",
    "        key_padding_mask = np.zeros((K,), dtype=np.bool_)\n",
    "        if pad > 0:\n",
    "            key_padding_mask[-pad:] = True\n",
    "\n",
    "        loss_mask = np.zeros((K,), dtype=np.bool_)\n",
    "        loss_mask[:valid_len] = True\n",
    "\n",
    "        states_b.append(s)\n",
    "        prev_a_b.append(prev)\n",
    "        target_a_b.append(a)\n",
    "        rtg_b.append(r)\n",
    "        t_b.append(tt)\n",
    "        keymask_b.append(key_padding_mask)\n",
    "        lossmask_b.append(loss_mask)\n",
    "\n",
    "    batch = {\n",
    "        \"states\": torch.tensor(np.stack(states_b), dtype=torch.long, device=device),            # (B,K)\n",
    "        \"prev_actions\": torch.tensor(np.stack(prev_a_b), dtype=torch.long, device=device),      # (B,K)\n",
    "        \"actions\": torch.tensor(np.stack(target_a_b), dtype=torch.long, device=device),         # (B,K)\n",
    "        \"rtg\": torch.tensor(np.stack(rtg_b), dtype=torch.float32, device=device),               # (B,K)\n",
    "        \"timesteps\": torch.tensor(np.stack(t_b), dtype=torch.long, device=device),              # (B,K)\n",
    "        \"key_padding_mask\": torch.tensor(np.stack(keymask_b), dtype=torch.bool, device=device), # (B,K)\n",
    "        \"loss_mask\": torch.tensor(np.stack(lossmask_b), dtype=torch.bool, device=device),       # (B,K)\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# ===================== 完整版 Decision Transformer（3 tokens / timestep）=====================\n",
    "class CausalTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, n_layer: int, dropout: float):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=n_layer)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        L = x.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        return self.enc(x, mask=causal_mask, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "class DecisionTransformerFull(nn.Module):\n",
    "    \"\"\"\n",
    "    每个时间步 3 tokens: [rtg_t, state_t, prev_action_t]\n",
    "    在 state token 位置输出 action logits（预测 action_t）\n",
    "    \"\"\"\n",
    "    def __init__(self, state_vocab: int, num_actions: int, max_timestep: int,\n",
    "                 d_model=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.action_pad = num_actions\n",
    "\n",
    "        self.state_emb = nn.Embedding(state_vocab, d_model)\n",
    "        self.action_emb = nn.Embedding(num_actions + 1, d_model)  # +PAD\n",
    "        self.time_emb = nn.Embedding(max_timestep + 1, d_model)\n",
    "        self.type_emb = nn.Embedding(3, d_model)  # 0=rtg,1=state,10_e=action\n",
    "\n",
    "        self.rtg_proj = nn.Linear(1, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.tr = CausalTransformer(d_model=d_model, n_head=n_head, n_layer=n_layer, dropout=dropout)\n",
    "        self.head = nn.Linear(d_model, num_actions)\n",
    "\n",
    "    def build_tokens(self, states, prev_actions, rtg, timesteps):\n",
    "        # states/prev_actions/rtg/timesteps: (B,K)\n",
    "        B, K = states.shape\n",
    "        d = self.state_emb.embedding_dim\n",
    "\n",
    "        # (B,K,D)\n",
    "        rtg_tok = self.rtg_proj(rtg.unsqueeze(-1))\n",
    "        s_tok = self.state_emb(states)\n",
    "        a_tok = self.action_emb(prev_actions)\n",
    "\n",
    "        # time embedding: 对三个 token 都加同一个 time_emb(t)\n",
    "        t_emb = self.time_emb(timesteps)  # (B,K,D)\n",
    "\n",
    "        rtg_tok = rtg_tok + t_emb + self.type_emb(torch.zeros((B, K), device=states.device, dtype=torch.long))\n",
    "        s_tok   = s_tok   + t_emb + self.type_emb(torch.ones((B, K), device=states.device, dtype=torch.long))\n",
    "        a_tok   = a_tok   + t_emb + self.type_emb(torch.full((B, K), 2, device=states.device, dtype=torch.long))\n",
    "\n",
    "        # interleave => (B, 3K, D)\n",
    "        x = torch.stack([rtg_tok, s_tok, a_tok], dim=2).reshape(B, 3 * K, d)\n",
    "        return x\n",
    "\n",
    "    def forward(self, states, prev_actions, rtg, timesteps, key_padding_mask=None):\n",
    "        # key_padding_mask: (B,K) -> expand to (B,3K)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(-1).expand(-1, -1, 3).reshape(key_padding_mask.size(0), -1)\n",
    "\n",
    "        x = self.build_tokens(states, prev_actions, rtg, timesteps)\n",
    "        x = self.drop(x)\n",
    "        h = self.tr(x, key_padding_mask=key_padding_mask)  # (B,3K,D)\n",
    "\n",
    "        # state token positions: 1,4,7,... => index 1 + 3*t\n",
    "        h_state = h[:, 1::3, :]  # (B,K,D)\n",
    "        logits = self.head(h_state)  # (B,K,A)\n",
    "        return logits\n",
    "\n",
    "model = DecisionTransformerFull(\n",
    "    state_vocab=state_vocab,\n",
    "    num_actions=NUM_ACTIONS,\n",
    "    max_timestep=max_timestep,\n",
    "    d_model=D_MODEL,\n",
    "    n_head=N_HEAD,\n",
    "    n_layer=N_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# ===================== 环境 Rollout 评估（每次输出一轮测试回报）=====================\n",
    "@torch.no_grad()\n",
    "def rollout_one_episode(model: DecisionTransformerFull, desired_return: float, horizon: int, K: int, rtg_scale: float):\n",
    "    model.eval()\n",
    "\n",
    "    params = make_params(batch_size=1, horizon=horizon + 1)\n",
    "    env = make_gridworld_env(params)\n",
    "\n",
    "    mat_state = [torch.tensor([get_state_id(env)], dtype=torch.long)]  # list of (1,)\n",
    "    baseline0 = env.weighted_traj_return(mat_state[:1], type=ALG_TYPE).float().view(-1)[0].item()\n",
    "\n",
    "    actions_dt = []\n",
    "    prev_action_dt = ACTION_PAD  # t=0 的 prev_action 用 PAD\n",
    "\n",
    "    for t in range(horizon):\n",
    "        # 当前 baselined return\n",
    "        cur_obj = env.weighted_traj_return(mat_state, type=ALG_TYPE).float().view(-1)[0].item()\n",
    "        cur_return = float(cur_obj - baseline0)\n",
    "        rtg_remaining = float(desired_return - cur_return)\n",
    "\n",
    "        # 构造 context（右侧 padding）\n",
    "        window_states = [int(x.view(-1)[0].item()) for x in mat_state]  # len=t+1\n",
    "        window_prev_actions = [ACTION_PAD] + actions_dt  # len=t+1，对齐 state_t 的 prev_action_t\n",
    "        window_len = min(len(window_states), K)\n",
    "\n",
    "        s_seq = np.array(window_states[-window_len:], dtype=np.int64)\n",
    "        pa_seq = np.array(window_prev_actions[-window_len:], dtype=np.int64)\n",
    "        rtg_seq = np.full((window_len,), rtg_remaining / float(rtg_scale), dtype=np.float32)\n",
    "        tt_seq = np.arange(t - window_len + 1, t + 1, dtype=np.int64)\n",
    "\n",
    "        pad = K - window_len\n",
    "        if pad > 0:\n",
    "            s_seq = np.pad(s_seq, (0, pad), constant_values=0)\n",
    "            pa_seq = np.pad(pa_seq, (0, pad), constant_values=ACTION_PAD)\n",
    "            rtg_seq = np.pad(rtg_seq, (0, pad), constant_values=0.0)\n",
    "            tt_seq = np.pad(tt_seq, (0, pad), constant_values=0)\n",
    "\n",
    "        key_padding_mask = np.zeros((K,), dtype=np.bool_)\n",
    "        if pad > 0:\n",
    "            key_padding_mask[-pad:] = True\n",
    "\n",
    "        states_t = torch.tensor(s_seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        prev_a_t = torch.tensor(pa_seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        rtg_t = torch.tensor(rtg_seq, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        ts_t = torch.tensor(tt_seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        kpm_t = torch.tensor(key_padding_mask, dtype=torch.bool, device=device).unsqueeze(0)\n",
    "\n",
    "        logits = model(states_t, prev_a_t, rtg_t, ts_t, key_padding_mask=kpm_t)  # (1,K,A)\n",
    "        # 取最后一个有效 timestep 的预测\n",
    "        action_dt = int(torch.argmax(logits[0, window_len - 1], dim=-1).item())\n",
    "        actions_dt.append(action_dt)\n",
    "\n",
    "        action_gw = int(DT2GW[torch.tensor(action_dt)].item())\n",
    "        env.step(int(t), torch.tensor([action_gw], dtype=torch.long))\n",
    "\n",
    "        mat_state.append(torch.tensor([get_state_id(env)], dtype=torch.long))\n",
    "\n",
    "    test_return = weighted_return_baselined(env, mat_state, alg_type=ALG_TYPE)\n",
    "    model.train()\n",
    "    return test_return\n",
    "\n",
    "# ===================== 训练（每 step 随机取一条轨迹；并周期性打印测试回报）=====================\n",
    "model.train()\n",
    "for step in range(1, TRAIN_STEPS + 1):\n",
    "    batch = sample_batch(trajs, BATCH_SIZE, K, RTG_SCALE, device)\n",
    "\n",
    "    logits = model(\n",
    "        states=batch[\"states\"],\n",
    "        prev_actions=batch[\"prev_actions\"],\n",
    "        rtg=batch[\"rtg\"],\n",
    "        timesteps=batch[\"timesteps\"],\n",
    "        key_padding_mask=batch[\"key_padding_mask\"],\n",
    "    )  # (B,K,A)\n",
    "\n",
    "    # 只在有效 token 上算 loss（排除 padding）\n",
    "    mask = batch[\"loss_mask\"] & (~batch[\"key_padding_mask\"])  # (B,K)\n",
    "    logits_flat = logits[mask]                 # (N,A)\n",
    "    targets_flat = batch[\"actions\"][mask]      # (N,)\n",
    "\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "\n",
    "    if step % EVAL_EVERY == 0:\n",
    "        test_ret = rollout_one_episode(model, desired_return=DESIRED_RETURN, horizon=EVAL_HORIZON, K=K, rtg_scale=RTG_SCALE)\n",
    "        print(f\"[TRAIN] step={step:5d} loss={float(loss.item()):.6f}  test_return={test_ret:.6f}\")\n",
    "\n",
    "# 保存\n",
    "ckpt = {\n",
    "    \"model\": model.state_dict(),\n",
    "    \"cfg\": {\n",
    "        \"CACHE_OUT\": str(CACHE_OUT),\n",
    "        \"NUM_ACTIONS\": NUM_ACTIONS,\n",
    "        \"K\": K,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"TRAIN_STEPS\": TRAIN_STEPS,\n",
    "        \"LR\": LR,\n",
    "        \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "        \"D_MODEL\": D_MODEL,\n",
    "        \"N_HEAD\": N_HEAD,\n",
    "        \"N_LAYER\": N_LAYER,\n",
    "        \"DROPOUT\": DROPOUT,\n",
    "        \"RTG_SCALE\": RTG_SCALE,\n",
    "        \"SEED\": SEED,\n",
    "        \"ALG_TYPE\": ALG_TYPE,\n",
    "        \"EVAL_EVERY\": EVAL_EVERY,\n",
    "        \"EVAL_HORIZON\": EVAL_HORIZON,\n",
    "        \"DESIRED_RETURN\": DESIRED_RETURN,\n",
    "    },\n",
    "    \"meta\": {\"alg_type(cache)\": cache.get(\"alg_type\"), \"state_vocab\": state_vocab, \"max_timestep\": max_timestep},\n",
    "}\n",
    "torch.save(ckpt, SAVE_PATH)\n",
    "print(\"[OK] saved:\", SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d834c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
