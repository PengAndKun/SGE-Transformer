import numpy as np
import torch
import torch.nn.functional as F
import pybullet as p
import time
import os

# å¯¼å…¥ä½ çš„ç¯å¢ƒã€æ¨¡å‹ç±»å’Œå®åŠ¨ä½œç©ºé—´
from ISE_Transformer.envs.coverage_visibility_pointcloud_aviary_optimized_add_Control import CoverageAviary


# å‡è®¾ä½ çš„å®šä¹‰åœ¨ä¹‹å‰çš„æ–‡ä»¶ä¸­ï¼Œè¿™é‡Œéœ€è¦ç¡®ä¿èƒ½å¼•ç”¨åˆ°
from ISE_Transformer.experiment.ise_pybullet.transformerbc import HuggingFaceTransformerBCNetwork
from ISE_Transformer.experiment.ise_pybullet.extract_trajectories_macro import MacroActionSpace27

def agent_performance(model_path, num_test_episodes=5):
    # --- 1. å‚æ•°é…ç½® (å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´) ---
    STATE_DIM = 6
    NUM_ACTIONS = 27
    H = 8  # è§‚å¯Ÿçš„å†å²é•¿åº¦
    MOVE_DISTANCE = 0.5

    # å½’ä¸€åŒ–å‚æ•° (å»ºè®®ä¸è®­ç»ƒä»£ç ä¸­ä½¿ç”¨çš„ç¼©æ”¾æ¯”ä¾‹ä¸€è‡´)
    POS_SCALE = 25.0
    RPY_SCALE = 3.14

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --- 2. åŠ è½½æ¨¡å‹ ---
    network = HuggingFaceTransformerBCNetwork(
        state_dim=STATE_DIM,
        num_actions=NUM_ACTIONS,
        max_seq_length=H
    ).to(DEVICE)

    checkpoint = torch.load(model_path, map_location=DEVICE)
    network.load_state_dict(checkpoint['network_state_dict'])
    network.eval()
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")

    # --- 3. åˆå§‹åŒ–ç¯å¢ƒ ---
    env = CoverageAviary(
        gui=True,  # å¼€å¯ç•Œé¢è§‚å¯Ÿ
        obstacles=True,
        num_rays=120,
        radar_radius=8.0,
        grid_res=0.5
    )
    action_space = MacroActionSpace27(move_distance=MOVE_DISTANCE)

    obs, info = env.reset()
    start_snapshot = env.get_snapshot()

    for episode in range(num_test_episodes):

        env.restore_snapshot(start_snapshot)

        # åˆå§‹åŒ–çŠ¶æ€å†å²é˜Ÿåˆ— (ç”¨äº Transformer è¾“å…¥)
        # åˆå§‹çŠ¶æ€ï¼š[x, y, z, r, p, y]
        init_state = np.concatenate([env.pos[0], env.rpy[0]])
        state_history = [init_state] * H  # åˆå§‹ç”¨èµ·å§‹çŠ¶æ€å¡«æ»¡çª—å£

        total_reward = 0.0
        done = False
        step_count = 0
        max_steps = 150  # è®¾ç½®ä¸€ä¸ªæœ€å¤§æµ‹è¯•æ­¥æ•°

        print(f"\n--- Episode {episode + 1} Start ---")

        while not done and step_count < max_steps:
            # --- 4. å‡†å¤‡ Transformer è¾“å…¥ ---
            # å–æœ€è¿‘çš„ H ä¸ªçŠ¶æ€å¹¶è¿›è¡Œå½’ä¸€åŒ–
            input_states = np.array(state_history[-H:], dtype=np.float32)
            input_states[:, :3] /= POS_SCALE
            input_states[:, 3:] /= RPY_SCALE

            # è½¬ä¸º Tensor (Batch_size=1, Seq_len=H, State_dim=6)
            input_tensor = torch.FloatTensor(input_states).unsqueeze(0).to(DEVICE)

            # --- 5. æ¨¡å‹é¢„æµ‹åŠ¨ä½œ ---
            with torch.no_grad():
                logits = network(input_tensor)
                probs = F.softmax(logits, dim=-1)
                # è¯„ä¼°æ—¶é€šå¸¸ä½¿ç”¨ deterministic (argmax)
                action_id = torch.argmax(probs, dim=-1).item()

            # --- 6. åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ (å¹³ç§»é€»è¾‘) ---
            # è¿™é‡Œè°ƒç”¨ä½ ä¹‹å‰çš„ execute_translation_step é€»è¾‘
            # ä¸ºäº†ç®€åŒ–ï¼Œç›´æ¥åœ¨æµ‹è¯•è„šæœ¬é‡Œå¿«é€Ÿå®ç°å•æ­¥å¹³ç§»
            start_pos = env.pos[0].copy()
            displacement = action_space.get_displacement(action_id)
            target_pos = start_pos + displacement

            # ç®€å•æ£€æŸ¥é™é«˜ (ä¸è®­ç»ƒé€»è¾‘å¯¹é½)
            if 0.5 <= target_pos[2] <= 3.5:
                # ç¬ç§»å¹¶æ‰«æ
                p.resetBasePositionAndOrientation(env.DRONE_IDS[0], target_pos, [0, 0, 0, 1],
                                                  physicsClientId=env.CLIENT)
                env._updateAndStoreKinematicInformation()
                _, reward, terminated, truncated, _ = env.compute_scan_at_pos(target_pos)

                total_reward += reward
                if terminated:
                    print(f"ğŸ’¥ ç¢°æ’å‘ç”Ÿ! æ­¥æ•°: {step_count}")
                    done = True
            else:
                print(f"âš ï¸ åŠ¨ä½œè¶Šç•Œ(é™é«˜æ‹¦æˆª)! Action: {action_id}")
                reward = 0
                done = True  # æˆ–è€…è·³è¿‡è¯¥åŠ¨ä½œ

            # --- 7. æ›´æ–°çŠ¶æ€å†å² ---
            current_state = np.concatenate([env.pos[0], env.rpy[0]])
            state_history.append(current_state)

            step_count += 1
            if step_count % 10 == 0:
                print(f"Step {step_count} | å½“å‰ç´¯ç§¯è¦†ç›–å¥–åŠ±: {total_reward:.2f}")

            time.sleep(0.02)  # ç¨å¾®å‡é€Ÿæ–¹ä¾¿è§‚å¯Ÿ

        print(
            f"ğŸ Episode {episode + 1} ç»“æŸ | æ€»å¾—åˆ†: {total_reward:.2f} | è¦†ç›–ç‡: {env._computeInfo()['coverage_ratio']:.2%}")

    env.close()


if __name__ == "__main__":
    MODEL_FILE = "sge_bc_model.pth"
    if os.path.exists(MODEL_FILE):
        agent_performance(MODEL_FILE)
    else:
        print("æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ã€‚")